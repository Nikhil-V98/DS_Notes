{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imp LiLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Import\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# magic function is to enable the inline plotting\n",
    "%matplotlib inline \n",
    "\n",
    "# for display all the column in the datafarmes\n",
    "pd.pandas.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Pandas Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "df= pd.read_csv(\"2019.csv\")\n",
    "# To show first 5 rows of data \n",
    "df.head()\n",
    "# To get shape of data\n",
    "df.shape\n",
    "# describe basic statistics of data (including cat and num)\n",
    "df.describe(include='all')\n",
    "# information about data frame\n",
    "df.info()\n",
    "# data types\n",
    "df.dtypes\n",
    "# To copy DataBase  \n",
    "data = df.copy(deep =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Name List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To seperate categorical and numerical columns\n",
    "features = df.columns\n",
    "cat = df.select_dtypes(include= ['object','category'])\n",
    "num =df.select_dtypes(exclude = ['object','category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Values dataframe\n",
    "list_unique = list((i,df[i].dtype,df[i].nunique(),df[i].unique()) for i in df.columns)\n",
    "pd_unique = pd.DataFrame(list_unique,columns = ['Column','Data type','No of Unique Values','Unique Values'],index = None)\n",
    "pd_unique\n",
    "\n",
    "# To get unique value count\n",
    "df['features_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Representation of Data (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.style.available) # To print the available plot style\n",
    "\n",
    "plt.style.use('style name') # To select Plot style\n",
    "\n",
    "plt.savefig('plot.png') # Save plot to PNG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commands for Basic Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y1, color = '#444444', label = 'y1 Feature Name') # X for feature name y1 for target label for labeling for the legend \n",
    "plt.plot(x, y2, color= '#5a7d9a', label = 'y2 Feature name') # X for feature name y2 for target label for labeling for the legend \n",
    "\n",
    "plt.fill_between(x,y1,alpha=0.25) # To fill area under line (line:x,y1)\n",
    "\n",
    "plt.legend() # To give legend for plot\n",
    "\n",
    "plt.title('Title name') # Title for plot\n",
    "plt.ylabel('Target name') # Y axis label\n",
    "plt.xlabel('feature name') # X axis Label \n",
    "\n",
    "plt.grid(True) # To Apply grid\n",
    "plt.tight_layout() # To auto adjust layout\n",
    "\n",
    "plt.show() # To show or print plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commands for Basic Vertical bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For side by side by bar plot\n",
    "\n",
    "# To divide bar of each column by width\n",
    "x_indexes = np.arange(len(x_feature))\n",
    "width = 0.25 \n",
    "\n",
    "plt.bar(x_indexes - width, y1, width=width,color = '#444444', label = 'y1 Feature Name') # If side by side plot not needed then replace 'x_indexes - width' with 'x_feature'\n",
    "plt.bar(x_indexes, y2,width=width,color= '#5a7d9a', label = 'y2 Feature name') \n",
    "plt.bar(x_indexes + width, y2, width=width, color= '#e5ae38', label = 'y2 Feature name')\n",
    "\n",
    "plt.legend() # To give legend for plot\n",
    "\n",
    "plt.xticks(ticks=x_indexes,labels= x_feature) # To replace X values\n",
    "\n",
    "plt.title('Title name') # Title for plot\n",
    "plt.ylabel('Target name') # Y axis label\n",
    "plt.xlabel('feature name') # X axis Label \n",
    "\n",
    "plt.grid(True) # To Apply grid\n",
    "plt.tight_layout() # To auto adjust layout\n",
    "\n",
    "plt.show() # To show or print plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commands for Basic Horizontal bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to vertical only chnaging \".bar\" to \".barh\"\n",
    "\n",
    "plt.barh(x, y, label = 'y1 Feature Name') \n",
    "\n",
    "plt.legend() # To give legend for plot\n",
    "\n",
    "plt.title('Title name') # Title for plot\n",
    "plt.ylabel('Target name') # Y axis label\n",
    "plt.xlabel('feature name') # X axis Label \n",
    "\n",
    "plt.grid(True) # To Apply grid\n",
    "plt.tight_layout() # To auto adjust layout\n",
    "\n",
    "plt.show() # To show or print plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bins = [values for bins] ex:[10,20,30,40,50]for age bin\n",
    "plt.hist(x, bins=5, edgecolor = \"black\") \n",
    "\n",
    "plt.legend() # To give legend for plot\n",
    "\n",
    "plt.title('Title name') # Title for plot\n",
    "plt.ylabel('Target name') # Y axis label\n",
    "plt.xlabel('feature name') # X axis Label \n",
    "\n",
    "plt.grid(True) # To Apply grid\n",
    "plt.tight_layout() # To auto adjust layout\n",
    "\n",
    "plt.show() # To show or print plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, s =100, c='green', marker = 'X',edgecolors='black') # s : size, c:color \n",
    "\n",
    "plt.title('Title name') # Title for plot\n",
    "plt.ylabel('Target name') # Y axis label\n",
    "plt.xlabel('feature name') # X axis Label \n",
    "\n",
    "plt.tight_layout() # To auto adjust layout\n",
    "\n",
    "plt.show() # To show or print plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we ar definning subplot with 2 rows and 1 column(It's like dividing plot into 2 horizontaly)\n",
    "# fig for number of figures\n",
    "# (ax1,ax2) for plot name we need provide appropriate number of name based on nrows and ncols\n",
    "fig, (ax1,ax2) = plt.subplot(nrows = 2, ncols = 1)\n",
    "# TO get different plots in different figures\n",
    "\n",
    "# fig1, ax1 = plt.subplot(nrows = 2, ncols = 1)\n",
    "# fig2, ax2 = plt.subplot(nrows = 2, ncols = 1) \n",
    "\n",
    "ax1.plot(x, y1, color = '#444444', label = 'y1 Feature Name') # X and Y for ax1 \n",
    "ax2.plot(x, y2, color= '#5a7d9a', label = 'y2 Feature name') # X and Y for ax2 \n",
    "ax2.plot(x, y3, color = '#444444', label = 'y1 Feature Name') # X and Y for ax2 \n",
    "\n",
    "ax1.legend() # To give legend for ax1 plot\n",
    "ax1.set_title('Title name') # Title for ax1 plot\n",
    "ax1.set_ylabel('Target name') # Y axis ax1 label\n",
    "ax1.set_xlabel('feature name') # X axis ax1 Label \n",
    "\n",
    "ax2.legend() # To give legend for ax2 plot\n",
    "ax2.set_title('Title name') # Title for ax2 plot\n",
    "ax2.set_ylabel('Target name') # Y axis ax2 label\n",
    "ax2.set_xlabel('feature name') # X axis ax2 Label\n",
    "\n",
    "plt.tight_layout() # To auto adjust layout\n",
    "\n",
    "plt.show() # To show or print plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below code used to create plt which show's relation between missing value and target varibales \n",
    "###### Missing value are treted as 1 and non missing value as 0 \n",
    "###### It will group missing and non-missing value and gives mean/median/ mode value of tat group \n",
    "###### Based on this we can see if the missing value is important or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot diagram\n",
    "data = df.copy()\n",
    "for feature in features_with_null:\n",
    "    \n",
    "    # lets a make variable that indicate 1 if the observaation is missing or 0 otherwise\n",
    "    data[feature] = np.where(data[feature].isnull(),1,0)\n",
    "    \n",
    "    # Lets calculate the mean SalePrice where the information is missing or present\n",
    "    data.groupby(feature)['target value'].median().plot.bar()\n",
    "    plt.title(feature)\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"if the median of missing value is high then we can conclude that thier is some coonction \n",
    "between missing and target and we can't directly replace nan with mode or median\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze categorical Columns:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(4,2,figsize=(12,15))\n",
    "for idx,cat_col in enumerate(cat.columns):\n",
    "    row,col = idx//2,idx%2\n",
    "    sns.countplot(x=cat_col,data=df,hue='traget_feature',ax=axes[row,col])\n",
    "\n",
    "plt.subplots_adjust(hspace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To analyze Numerical Columns:\n",
    "\n",
    "fig,axes = plt.subplots(1,3,figsize=(17,5))\n",
    "for idx,cat_col in enumerate(num.columns):\n",
    "    sns.boxplot(y=cat_col,data=df,x='traget_feature',ax=axes[idx])\n",
    "\n",
    "print(df[num.columns].describe())\n",
    "plt.subplots_adjust(hspace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-Relation Heatmap\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', annot=True, fmt='.1f', linewidths=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot\n",
    "for var in ['features_name']:\n",
    "    sns.boxplot(df[var])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for seeing distrubution of data\n",
    "for var in ['feature names']:\n",
    "    fig= plt.subplots(figsize=(12, 8))\n",
    "    sns.distplot(df[var])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "df.groupby('features_name')['traget_feature'].median().plot()\n",
    "plt.xlabel('features_name')\n",
    "plt.ylabel('Median features_name')\n",
    "plt.title(\"features_name vs traget_feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Completely at Random(MCAR):\n",
    "\"\"\" A variable is missing completely at random (MCAR)if the missing values on a given variable (Y) don't have a relationship with other \n",
    "variables in a given data set or with the variable (Y) itself. In other words, When data is MCAR, there is no relationship between the data\n",
    "missing and any values, and there is no particular reason for the missing values.\"\"\"\n",
    "\n",
    "# Missing at Random(MAR):\n",
    "\"\"\" Let's understands the following examples:\n",
    "Women are less likely to talk about age and weight than men.\n",
    "Men are less likely to talk about salary and emotions than women.\n",
    "familiar right?… This sort of missing content indicates missing at random.\n",
    "\n",
    "MAR occurs when the missingness is not random, but there is a systematic relationship between missing values and other observed data but not\n",
    "the missing data.\n",
    "Let me explain to you: you are working on a dataset of ABC survey. You will find out that many emotion observations are null.\n",
    "You decide to dig deeper and found most of the emotion observations are null that belongs to men's observation.\"\"\"\n",
    "\n",
    "# Missing Not at Random(MNAR):\n",
    "\"\"\" The final and most difficult situation of missingness. MNAR occurs when the missingness is not random, and there is a systematic relationship\n",
    "between missing value, observed value, and missing itself. To make sure, If the missingness is in 2 or more variables holding the same pattern,\n",
    "you can sort the data with one variable and visualize it.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting missing data\n",
    "mis_val =df.isna().sum()\n",
    "mis_val_per = df.isna().sum()/len(df)*100\n",
    "mis_val_table = pd.concat([mis_val, mis_val_per], axis=1)\n",
    "mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,:] != 0].sort_values('% of Total Values', ascending=False).round(1)\n",
    "mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get column name which has null value\n",
    "na_features = [var for var in df.columns if df[var].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding reason for missing data using plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Graphical view of missing data\n",
    "\"\"\"The msno.matrix() is a nullity matrix that will help to visualize the location of the null observations.\"\"\"\n",
    "\n",
    "import missingno as msno\n",
    "msno.matrix(df)\n",
    "\n",
    "# To sort graph using one value\n",
    "\"\"\"The missingno package additionally lets us sort the chart by a selective column. Let's sort the value by one feature name \n",
    "column to detect if there is a pattern in the missing values.\"\"\"\n",
    "\n",
    "sorted = df.sort_values('feature_name')\n",
    "msno.matrix(sorted)\n",
    "\n",
    "# Heatmap for Missing Value\n",
    "\"\"\"msno. heatmap() helps to visualize the correlation between missing features. \n",
    "The missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the \n",
    "presence of another\n",
    "\n",
    "Nullity correlation ranges from -1 (if one variable appears the other definitely does not) to 0 (variables appearing or not \n",
    "appearing have no effect on one another) to 1 (if one variable appears the other definitely also does)\"\"\"\n",
    "\n",
    "msno.heatmap(df)\n",
    "\n",
    "# Dendrogram for missing value\n",
    "\n",
    "msno.dendrogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Case Analysis(CCA):\n",
    "\"\"\"This is a quite straightforward method of handling the Missing Data, which directly removes the rows that have missing data \n",
    "i.e we consider only those rows where we have complete data i.e data is not missing. This method is also \n",
    "popularly known as “Listwise deletion”.\n",
    "\n",
    "When to Use:-\n",
    "> Data is MAR(Missing At Random).\n",
    "> Good for Mixed, Numerical, and Categorical data.\n",
    "> Missing data is not more than 5% - 6% of the dataset.\n",
    "> Data doesn't contain much information and will not bias the dataset.\n",
    "\"\"\"\n",
    "\n",
    "df.dropna(subset=['fature_name'],how='any',axis = 0) # Drop rows which contains any NaN or missing value for feature_name column & for complete df remove subset and how\n",
    "\n",
    "## Imputations Techniques for non Time Series Problems:\n",
    "\n",
    "# Arbitrary Value Imputation\n",
    "\"\"\"This is an important technique used in Imputation as it can handle both the Numerical and Categorical variables. \n",
    "This technique states that we group the missing values in a column and assign them to a new value that is far away from the range of that column.\n",
    "Mostly we use values like 99999999 or -9999999 or “Missing” or “Not defined” for numerical & categorical variables.\n",
    "\n",
    "When to Use:-\n",
    "> When data is not MAR(Missing At Random).\n",
    "> Suitable for All.\n",
    "\"\"\"\n",
    "\n",
    "df['features_name'].fillna('Missing')\n",
    "\n",
    "\n",
    "# Frequent Category Imputation\n",
    "\"\"\"\"This technique says to replace the missing value with the variable with the highest frequency or in simple words replacing the values \n",
    "with the Mode of that column. This technique is also referred to as Mode\n",
    "\n",
    "When to Use:-\n",
    "> Data is Missing at Random(MAR)\n",
    "> Missing data is not more than 5% - 6% of the dataset.\n",
    "\"\"\"\n",
    "\n",
    "df['features_name'].fillna(df['features_name'].mode()[0])\n",
    "df['features_name'].fillna(df['features_name'].mean())\n",
    "\n",
    "## Imputations Techniques for Time Series Problems:\n",
    "\n",
    "# Imputing using ffill\n",
    "\n",
    "df.fillna(method='ffill')\n",
    "\n",
    "# Imputation using bfill\n",
    "\n",
    "df.fillna(method='bfill')\n",
    "\n",
    "# Imputation using Linear Interpolation method\n",
    "\n",
    "\"\"\"Linear interpolation is an imputation technique that assumes a linear relationship between data points and utilises non-missing values\n",
    " from adjacent data points to compute a value for a missing data point.\"\"\"\n",
    "\n",
    "df.interpolate(limit_direction=\"both\")\n",
    "\n",
    "## Advanced Imputation Techniques:\n",
    "\n",
    "# Imputation Using k-NN\n",
    "\"\"\"The k nearest neighbours is an algorithm that is used for simple classification. The algorithm uses 'feature similarity' to predict \n",
    "the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the\n",
    "training set. This can be very useful in making predictions about the missing values by finding the k's closest neighbours to the \n",
    "observation with missing data and then imputing them based on the non-missing values in the neighbourhood.\n",
    "\n",
    "The fundamental weakness of KNN doesn't work on categorical features. We need to convert them into numeric using any encoding method. \n",
    "It requires normalizing data as KNN Imputer is a distance-based imputation method and different scales of data generate biased replacements \n",
    "for the missing values.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "df['feature_name'] = knn_imputer.fit_transform(df[['feature_name']])\n",
    "df = pd.DataFrame(knn_imputer.fit_transform(df),columns = df.columns) # for whole dataset\n",
    "\n",
    "# Imputation Using Multivariate Imputation by Chained Equation(MICE)\n",
    "\"\"\"This type of imputation works by filling the missing data multiple times. Multiple Imputations (MIs) are much better than a single \n",
    "imputation as it measures the uncertainty of the missing values in a better way. The chained equations approach is also very flexible and can\n",
    " handle different variables of different data types (ie., continuous or binary) as well as complexities such as bounds or survey skip patterns. \n",
    "\"\"\"\n",
    "\n",
    "from impyute.imputation.cs import mice\n",
    "imputed_training=mice(df.values)\n",
    "\n",
    "# In sklearn, it is implemented as follows\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "mice_imputer = IterativeImputer()\n",
    "df['feature_name'] = mice_imputer.fit_transform(df[['feature_name']])\n",
    "\n",
    "# Stochastic regression imputation:\n",
    "\"\"\"It is quite similar to regression imputation which tries to predict the missing values by regressing it from other related variables\n",
    " in the same dataset plus some random residual value.\"\"\"\n",
    "\n",
    "# Extrapolation and Interpolation:\n",
    "\"\"\"It tries to estimate values from other observations within the range of a discrete set of known data points.\"\"\"\n",
    "\n",
    "# Hot-Deck imputation:\n",
    "\"\"\"Works by randomly choosing the missing value from a set of related and similar variables.\"\"\"\n",
    "\n",
    "# Cold-Deck Imputation: \n",
    "\"\"\"A systematically chosen value from an individual who has similar values on other variables.This is similar to Hot Deck in most ways,\n",
    " but removes the random variation.\"\"\"\n",
    "\n",
    " # Replacing Values using backward & forward filling \n",
    "\n",
    "df.fillna(method = 'bfill', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot outlier detection\n",
    "\n",
    "for var in 'feature name list':\n",
    "    sns.boxplot(df[var])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Treatment\n",
    "\n",
    "# DELETING OBSERVATIONS:\n",
    "\"\"\"We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. \n",
    "We can also use trimming at both ends to remove outliers. But deleting the observation is not a good idea when we have small dataset.\"\"\"\n",
    "\n",
    "for var in 'list of feature name' :\n",
    "    Q1 = df[var].quantile(0.25)\n",
    "    Q3 = df[var].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "df = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "\n",
    "# TRANSFORMING VALUES:\n",
    "\n",
    "# Scalling\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "df['feature_name'] = scaler.fit_transform(df['feature_name'].values.reshape(-1,1))\n",
    "\n",
    "# Log Transformation\n",
    "df['feature_name'] = np.log(df['feature_name'])\n",
    "\n",
    "# Box-transformation\n",
    "import scipy\n",
    "df['feature_name'],fitted_lambda= scipy.stats.boxcox(df['feature_name'] ,lmbda=None)\n",
    "\n",
    "# IMPUTATION\n",
    "q1 = df['feature_name'].quantile(0.25)\n",
    "q3 = df['feature_name'].quantile(0.75)\n",
    "iqr = q3-q1\n",
    "Lower_tail = q1 - 1.5 * iqr\n",
    "Upper_tail = q3 + 1.5 * iqr\n",
    "m = np.mean(df['feature_name']) # Mean,median,mode\n",
    "for i in df['feature_name']:\n",
    "    if i > Upper_tail or i < Lower_tail:\n",
    "            df['feature_name'] = df['feature_name'].replace(i, m)\n",
    "\n",
    "# Binning\n",
    "\"\"\" Binning the data and categorizing them will totally avoid the outliers. It will make the data categorical instead.\"\"\"\n",
    "\n",
    "df['feature_name'] = pd.cut(df['feature_name'], bins = [0, 10, 20, 30, 40, 55], labels = ['Very Low', 'Low', 'Average', 'High', 'Very High'])\n",
    "\n",
    "# Quantile based flooring & capping\n",
    "\n",
    "q10 = df['feature_name'].quantile(0.10)\n",
    "q90 = df['feature_name'].quantile(0.90)\n",
    "df[\"feature_name\"] = np.where(df[\"feature_name\"] <q10, q10,df['feature_name'])\n",
    "df[\"feature_name\"] = np.where(df[\"feature_name\"] >q90, q90,df['feature_name'])\n",
    "\n",
    "\n",
    "# Winsorizing\n",
    "\"\"\"Unlike trimming, here we replace the outliers with other values. Common is replacing the outliers on the upper side with 95% percentile\n",
    " value and outlier on the lower side with 5% percentile.\"\"\"\n",
    "\n",
    "import scipy.stats\n",
    "scipy.stats.mstats.winsorize(df['feature_name'],limits=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding using map function\n",
    "feature_name_dict = {'N':0, 'Y':1}\n",
    "df['feature_name'] = df['feature_name'].map(feature_name_dict)\n",
    "\n",
    "# Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "feature_col = \"list of feature names\"\n",
    "le = LabelEncoder()\n",
    "for col in feature_col:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# One Hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(sparse=False,drop = 'if_binary')\n",
    "transformed_data = onehotencoder.fit_transform(df[['feature_name']])\n",
    "# the above transformed_data is an array so convert it to dataframe and add feature name to the comlum\n",
    "encoded_data = pd.DataFrame(transformed_data, columns=onehotencoder.get_feature_names_out())\n",
    "# now concatenate the original data and the encoded data using pandas\n",
    "df = pd.concat([df, encoded_data], axis=1).drop('feature_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "na_features = ['feature names']\n",
    "for var in na_features:\n",
    "    df[var] = scaler.fit_transform(df[var].values.reshape(-1, 1))\n",
    "    df_test[var] = scaler.transform(df_test[var].values.reshape(-1, 1))\n",
    "\n",
    "# maximum absolute scaling\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "max_scaler = MaxAbsScaler()\n",
    "for var in na_features:\n",
    "    df[var] = max_scaler.fit_transform(df[var].values.reshape(-1, 1))\n",
    "    df_test[var] = max_scaler.transform(df_test[var].values.reshape(-1, 1))  \n",
    "\n",
    "#  min-max feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm_scaler = MinMaxScaler()\n",
    "for var in na_features:\n",
    "    df[var] = mm_scaler.fit_transform(df[var].values.reshape(-1, 1))\n",
    "    df_test[var] = mm_scaler.transform(df_test[var].values.reshape(-1, 1))  \n",
    "\n",
    "# log scaling  \n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "ln = FunctionTransformer(np.log1p)\n",
    "for var in na_features:\n",
    "    df[var] = ln.fit_transform(df[var].values.reshape(-1, 1))\n",
    "    df_test[var] = ln.transform(df_test[var].values.reshape(-1, 1))\n",
    "\n",
    "# Normalize\n",
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer()\n",
    "for var in na_features:\n",
    "    df[var] = norm.fit_transform(df[var].values.reshape(-1, 1))\n",
    "    df_test[var] = norm.transform(df_test[var].values.reshape(-1, 1))\n",
    "\n",
    "# RobustScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robu = RobustScaler()\n",
    "for var in na_features:\n",
    "    df[var] = robu.fit_transform(df[var].values.reshape(-1, 1))\n",
    "    df_test[var] = robu.transform(df_test[var].values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generlized Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(['Traget_feature'], axis=1)\n",
    "y = df[['Traget_feature']]\n",
    "X_test = df_test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr=linear_model.LinearRegression()\n",
    "lm_model=lr.fit(X_train,y_train)\n",
    "y_pred=lm_model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val,y_pred))\n",
    "\n",
    "# XGBRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "xgb_reg = XGBRegressor(n_estimators=400, max_depth = 20, learning_rate = 0.05,  min_child_weight  = 40)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_pred, y_val))\n",
    "print (xgb_reg)\n",
    "\n",
    "# RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "random_forest_regressor = RandomForestRegressor(n_estimators = 10, random_state = 27)\n",
    "random_forest_regressor.fit(X_train, y_train)\n",
    "Y_pred = random_forest_regressor.predict(X_val)\n",
    "mse = mean_squared_error(y_val, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "tree_model = DecisionTreeRegressor(random_state=1)\n",
    "tree_model.fit(X_train, y_train)\n",
    "Y_pred = tree_model.predict(X_val)\n",
    "mse = mean_squared_error(y_val, Y_pred)\n",
    "rmse = np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logistic_model = LogisticRegression(random_state=1)\n",
    "lg_model = logistic_model.fit(X_train,y_train)\n",
    "y_pred_logistic=lg_model.predict(X_val)\n",
    "score_logistic =accuracy_score(y_pred_logistic,y_val)*100\n",
    "\n",
    "# DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_model = DecisionTreeClassifier(random_state=1)\n",
    "tree_model.fit(X_train,y_train)\n",
    "pred_cv_tree=tree_model.predict(X_val)\n",
    "score_tree =accuracy_score(pred_cv_tree,y_val)*100 \n",
    "\n",
    "# RandomForestClassifier with grid_search\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "paramgrid = {'max_depth': list(range(1,20,2)),'n_estimators':list(range(1,200,20))}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=1),paramgrid)\n",
    "grid_search.fit(X_train,y_train)\n",
    "grid_search.best_estimator_\n",
    "\n",
    "forest_model = RandomForestClassifier(random_state=1,max_depth=10,n_estimators=50) # add values from grid_search.best_estimator_\n",
    "forest_model.fit(X_train,y_train)\n",
    "pred_cv_forest=forest_model.predict(X_val)\n",
    "score_forest = accuracy_score(pred_cv_forest,y_val)*100\n",
    "\n",
    "# XGBClassifier\n",
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier(n_estimators=50,max_depth=4) \n",
    "xgb_model.fit(X_train,y_train)\n",
    "pred_xgb=xgb_model.predict(X_val)\n",
    "score_xgb = accuracy_score(pred_xgb,y_val)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot feature importance\n",
    "def plot_features(booster, figsize):    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plot_features(model_name, (10,14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for saving all model performance in one place\n",
    "from collections import OrderedDict\n",
    "model_performance = OrderedDict()\n",
    "\n",
    "# This line for all model performance\n",
    "model_performance['Model name'] = round(rmse,3)\n",
    "print(f'Root Mean Squared Error of the model is : {round(rmse,3)}')\n",
    "\n",
    "# For showing model performance\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save data into other \n",
    "testRes = df[['feature_name']]\n",
    "testRes['feature_name'] = df['feature_name']\n",
    "testRes\n",
    "\n",
    "# To predict and save values in columns\n",
    "yPreds = model_name.predict(X_test)\n",
    "testRes['target_name'] = yPreds\n",
    "submission = testRes[['feature_name & Traget_name to be saved in file']]\n",
    "\n",
    "# To SAve submission file\n",
    "submission.columns = ['feature_name & Traget_name to be saved in file']\n",
    "submission.to_csv('submission.csv', index = False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.graphics.api import qqplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"filename.csv\") # This is dataset loading\n",
    "\n",
    "# Date time dtype conversion\n",
    "df['DateTime']=pd.to_datetime(df.Datetime, format='%d-%m-%Y %H:%M')\n",
    "\n",
    "# to specify index col afte loading if not mentioned while loading\n",
    "df1 = df.set_index('Datetime') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample Hourly Data to Daily Data\n",
    "df1.resample('D').sum()\n",
    "\n",
    "# To store index into a column\n",
    "df1['DateTime']=df1.index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Time Based Feature.This helps regression models to understand the trend in the data.\n",
    "\n",
    "# Below function extracts date related features from datetime\n",
    "def create_date_featues(df):\n",
    "    df1['Date'] = pd.to_datetime(df1['DateTime']).dt.year\n",
    "    df1['Year'] = pd.to_datetime(df1['DateTime']).dt.year\n",
    "    df1['Month'] = pd.to_datetime(df1['DateTime']).dt.month\n",
    "    df1['Day'] = pd.to_datetime(df1['DateTime']).dt.day\n",
    "    df1['Dayofweek'] = pd.to_datetime(df1['DateTime']).dt.dayofweek\n",
    "    df1['DayOfyear'] = pd.to_datetime(df1['DateTime']).dt.dayofyear\n",
    "    df1['Week'] = pd.to_datetime(df1['DateTime']).dt.week\n",
    "    df1['Quarter'] = pd.to_datetime(df1['DateTime']).dt.quarter \n",
    "    df1['Is_month_start'] = pd.to_datetime(df1['DateTime']).dt.is_month_start\n",
    "    df1['Is_month_end'] = pd.to_datetime(df1['DateTime']).dt.is_month_end\n",
    "    df1['Is_quarter_start'] = pd.to_datetime(df1['DateTime']).dt.is_quarter_start\n",
    "    df1['Is_quarter_end'] = pd.to_datetime(df1['DateTime']).dt.is_quarter_end\n",
    "    df1['Is_year_start'] = pd.to_datetime(df1['DateTime']).dt.is_year_start\n",
    "    df1['Is_year_end'] = pd.to_datetime(df1['DateTime']).dt.is_year_end\n",
    "    df1['Semester'] = np.where(df1['Quarter'].isin([1,2]),1,2)\n",
    "    df1['Is_weekend'] = np.where(df1['Dayofweek'].isin([5,6]),1,0)\n",
    "    df1['Is_weekday'] = np.where(df1['Dayofweek'].isin([0,1,2,3,4]),1,0)\n",
    "    df1['Hour'] = pd.to_datetime(df1['DateTime']).dt.hour\n",
    "    \n",
    "    return df\n",
    "\n",
    "# extracting time related \n",
    "df1=create_date_featues(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plots in the notebook\n",
    "%matplotlib inline\n",
    "df1['traget_feature'].plot(figsize=(12,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-correlations\n",
    "\n",
    "Before we decide which model to use, we need to look at auto-correlations.\n",
    "\n",
    "#### Autocorrelation correlogram. \n",
    "Seasonal patterns of time series can be examined via correlograms, which display graphically and numerically the autocorrelation function (ACF). Auto-correlation in pandas plotting and statsmodels graphics standardize the data before computing the auto-correlation. These libraries subtract the mean and divide by the standard deviation of the data.\n",
    "\n",
    "When using standardization, they make an assumption that your data has been generated with a Gaussian law (with a certain mean and standard deviation). This may not be the case in reality.\n",
    "\n",
    "Correlation is sensitive. Both (matplotlib and pandas plotting) of these functions have their drawbacks. The figure generated by the following code using matplotlib will be identical to figure generated by pandas plotting or statsmodels graphics.\n",
    "\n",
    "#### Partial autocorrelations. \n",
    "Another useful method to examine serial dependencies is to examine the partial autocorrelation function (PACF) – an extension of autocorrelation, where the dependence on the intermediate elements (those within the lag) is removed.\n",
    "\n",
    "Once we determine the nature of the auto-correlations we use the following rules of thumb.\n",
    "\n",
    "   * Rule 1: If the ACF shows exponential decay, the PACF has a spike at lag 1, and no correlation for other lags, then use one autoregressive (p)parameter\n",
    "   * Rule 2: If the ACF shows a sine-wave shape pattern or a set of exponential decays, the PACF has spikes at lags 1 and 2, and no correlation for other lags, the use two autoregressive (p) parameters\n",
    "   * Rule 3: If the ACF has a spike at lag 1, no correlation for other lags, and the PACF damps out exponentially, then use one moving average (q) parameter.\n",
    "   * Rule 4: If the ACF has spikes at lags 1 and 2, no correlation for other lags, and the PACF has a sine-wave shape pattern or a set of exponential decays, then use two moving average (q) parameter.\n",
    "   * Rule 5: If the ACF shows exponential decay starting at lag 1, and the PACF shows exponential decay starting at lag 1, then use one autoregressive (p) and one moving average (q) parameter.\n",
    "   \n",
    "#### Removing serial dependency. \n",
    "Serial dependency for a particular lag can be removed by differencing the series. There are two major reasons for such transformations.\n",
    "\n",
    "   * First, we can identify the hidden nature of seasonal dependencies in the series. Autocorrelations for consecutive lags are interdependent, so removing some of the autocorrelations will change other auto correlations, making other seasonalities more apparent.\n",
    "   * Second, removing serial dependencies will make the series stationary, which is necessary for ARIMA and other techniques.\n",
    "\n",
    "\n",
    "Another popular test for serial correlation is the Durbin-Watson statistic. The DW statistic will lie in the 0-4 range, with a value near two indicating no first-order serial correlation. Positive serial correlation is associated with DW values below 2 and negative serial correlation with DW values above 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.durbin_watson(df1['feature_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plots in the notebook\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(df1['feature_name'].values.squeeze(), lags=100, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(df1['feature_name'], lags=100, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "# show plots in the notebook\n",
    "%matplotlib inline\n",
    "df1['feature_name_copy'] = df1['feature_name']\n",
    "df1['feature_name_copy'] = (df1['feature_name_copy'] - df1['feature_name_copy'].mean()) / (df1['feature_name_copy'].std())\n",
    "plt.acorr(df1['feature_name_copy'],maxlags = len(df1['feature_name_copy']) -1, linestyle = \"solid\", usevlines = False, marker='')\n",
    "plt.show()\n",
    "autocorrelation_plot(df1['feature_name'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test split\n",
    "Timeseries problems requires time based validation instead of generaly used kfold validation in regression problem. Kfold splits the data randomly and checking the model accuracy by predicting on timeperiod 2016 by using 2017 data makes no sense.\n",
    "insted we use time based validation for the time period (2017-01-01 to 2017-04-01) of 4 months, since the test set contains 4 months data to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1=indexed_df[ :'2014-02-25 23:00:00']#Train period from 2016-01-01 to 2017-02-31\n",
    "val1=indexed_df['2014-02-25 23:00:00': ] #Month 3,4,5,6 as validtaion period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 aug:\n",
    "7pm:logistic\n",
    "8pm:pca\n",
    "8.45:time_series\n",
    "24 aug:\n",
    "8pm:SQL-ETL\n",
    "25 aug:\n",
    "8pm:SQL-ETL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling The model File for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(model_name,open('regmodel.pkl','wb')) # to save the pickle file\n",
    "\n",
    "pickled_model = pickle.load(open('regmodel.pkl','rb')) # To load the pickle file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "788a82b91c0336814844834dbe028694862d9bc55d1fd4a8a9273fa14df8fc57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
