{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# magic function is to enable the inline plotting\n",
    "%matplotlib inline \n",
    "\n",
    "# for display all the column in the datafarmes\n",
    "pd.pandas.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "df= pd.read_csv(\"2019.csv\")\n",
    "# To show first 5 rows of data \n",
    "df.head()\n",
    "# To get shape of data\n",
    "df.shape\n",
    "# describe basic statistics of data (including cat and num)\n",
    "df.describe(include='all')\n",
    "# information about data frame\n",
    "df.info()\n",
    "# data types\n",
    "df.dtypes\n",
    "# To seperate categorical and numerical columns\n",
    "features = df.columns\n",
    "cat = df.select_dtypes(include= ['object','category'])\n",
    "num =df.select_dtypes(exclude = ['object','category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Values\n",
    "for var in features:\n",
    "    if len(df[var].unique()) < 6 :\n",
    "        print(var,'-->',len(df[var].unique()),':', df[var].unique())\n",
    "    else :\n",
    "        print(var,'-->',len(df[var].unique()))\n",
    "\n",
    "# To get unique value count\n",
    "df['features_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze values assigned to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze categorical Columns:\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(4,2,figsize=(12,15))\n",
    "for idx,cat_col in enumerate(cat.columns):\n",
    "    row,col = idx//2,idx%2\n",
    "    sns.countplot(x=cat_col,data=df,hue='traget_feature',ax=axes[row,col])\n",
    "\n",
    "plt.subplots_adjust(hspace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To analyze Numerical Columns:\n",
    "\n",
    "fig,axes = plt.subplots(1,3,figsize=(17,5))\n",
    "for idx,cat_col in enumerate(num.columns):\n",
    "    sns.boxplot(y=cat_col,data=df,x='traget_feature',ax=axes[idx])\n",
    "\n",
    "print(df[num.columns].describe())\n",
    "plt.subplots_adjust(hspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Completely at Random(MCAR):\n",
    "\"\"\" A variable is missing completely at random (MCAR)if the missing values on a given variable (Y) don't have a relationship with other \n",
    "variables in a given data set or with the variable (Y) itself. In other words, When data is MCAR, there is no relationship between the data\n",
    "missing and any values, and there is no particular reason for the missing values.\"\"\"\n",
    "\n",
    "# Missing at Random(MAR):\n",
    "\"\"\" Let's understands the following examples:\n",
    "Women are less likely to talk about age and weight than men.\n",
    "Men are less likely to talk about salary and emotions than women.\n",
    "familiar right?… This sort of missing content indicates missing at random.\n",
    "\n",
    "MAR occurs when the missingness is not random, but there is a systematic relationship between missing values and other observed data but not\n",
    "the missing data.\n",
    "Let me explain to you: you are working on a dataset of ABC survey. You will find out that many emotion observations are null.\n",
    "You decide to dig deeper and found most of the emotion observations are null that belongs to men's observation.\"\"\"\n",
    "\n",
    "# Missing Not at Random(MNAR):\n",
    "\"\"\" The final and most difficult situation of missingness. MNAR occurs when the missingness is not random, and there is a systematic relationship\n",
    "between missing value, observed value, and missing itself. To make sure, If the missingness is in 2 or more variables holding the same pattern,\n",
    "you can sort the data with one variable and visualize it.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting missing data\n",
    "mis_val =df.isna().sum()\n",
    "mis_val_per = df.isna().sum()/len(df)*100\n",
    "mis_val_table = pd.concat([mis_val, mis_val_per], axis=1)\n",
    "mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "       mis_val_table_ren_columns.iloc[:,:] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get column name which has null value\n",
    "na_features = [var for var in df.columns if df[var].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding reason for missing data using matrix plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Graphical view of missing data\n",
    "\"\"\"The msno.matrix() is a nullity matrix that will help to visualize the location of the null observations.\"\"\"\n",
    "\n",
    "import missingno as msno\n",
    "msno.matrix(df)\n",
    "\n",
    "# To sort graph using one value\n",
    "\"\"\"The missingno package additionally lets us sort the chart by a selective column. Let's sort the value by one feature name \n",
    "column to detect if there is a pattern in the missing values.\"\"\"\n",
    "\n",
    "sorted = df.sort_values('feature_name')\n",
    "msno.matrix(sorted)\n",
    "\n",
    "# Heatmap for Missing Value\n",
    "\"\"\"msno. heatmap() helps to visualize the correlation between missing features. \n",
    "The missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the \n",
    "presence of another\n",
    "\n",
    "Nullity correlation ranges from -1 (if one variable appears the other definitely does not) to 0 (variables appearing or not \n",
    "appearing have no effect on one another) to 1 (if one variable appears the other definitely also does)\"\"\"\n",
    "\n",
    "msno.heatmap(df)\n",
    "\n",
    "# Dendrogram for missing value\n",
    "\n",
    "msno.dendrogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Case Analysis(CCA):\n",
    "\"\"\"This is a quite straightforward method of handling the Missing Data, which directly removes the rows that have missing data \n",
    "i.e we consider only those rows where we have complete data i.e data is not missing. This method is also \n",
    "popularly known as “Listwise deletion”.\n",
    "\n",
    "When to Use:-\n",
    "> Data is MAR(Missing At Random).\n",
    "> Good for Mixed, Numerical, and Categorical data.\n",
    "> Missing data is not more than 5% - 6% of the dataset.\n",
    "> Data doesn't contain much information and will not bias the dataset.\n",
    "\"\"\"\n",
    "\n",
    "df.dropna(subset=['fature_name'],how='any',axis = 0) # Drop rows which contains any NaN or missing value for feature_name column & for complete df remove subset and how\n",
    "\n",
    "## Imputations Techniques for non Time Series Problems:\n",
    "\n",
    "# Arbitrary Value Imputation\n",
    "\"\"\"This is an important technique used in Imputation as it can handle both the Numerical and Categorical variables. \n",
    "This technique states that we group the missing values in a column and assign them to a new value that is far away from the range of that column.\n",
    "Mostly we use values like 99999999 or -9999999 or “Missing” or “Not defined” for numerical & categorical variables.\n",
    "\n",
    "When to Use:-\n",
    "> When data is not MAR(Missing At Random).\n",
    "> Suitable for All.\n",
    "\"\"\"\n",
    "\n",
    "df['features_name'].fillna('Missing')\n",
    "\n",
    "\n",
    "# Frequent Category Imputation\n",
    "\"\"\"\"This technique says to replace the missing value with the variable with the highest frequency or in simple words replacing the values \n",
    "with the Mode of that column. This technique is also referred to as Mode\n",
    "\n",
    "When to Use:-\n",
    "> Data is Missing at Random(MAR)\n",
    "> Missing data is not more than 5% - 6% of the dataset.\n",
    "\"\"\"\n",
    "\n",
    "df['features_name'].fillna(df['features_name'].mode())\n",
    "\n",
    "## Imputations Techniques for Time Series Problems:\n",
    "\n",
    "# Imputing using ffill\n",
    "\n",
    "df.fillna(method='ffill')\n",
    "\n",
    "# Imputation using bfill\n",
    "\n",
    "df.fillna(method='bfill')\n",
    "\n",
    "# Imputation using Linear Interpolation method\n",
    "\n",
    "\"\"\"Linear interpolation is an imputation technique that assumes a linear relationship between data points and utilises non-missing values\n",
    " from adjacent data points to compute a value for a missing data point.\"\"\"\n",
    "\n",
    "df.interpolate(limit_direction=\"both\")\n",
    "\n",
    "## Advanced Imputation Techniques:\n",
    "\n",
    "# Imputation Using k-NN\n",
    "\"\"\"The k nearest neighbours is an algorithm that is used for simple classification. The algorithm uses 'feature similarity' to predict \n",
    "the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the\n",
    "training set. This can be very useful in making predictions about the missing values by finding the k's closest neighbours to the \n",
    "observation with missing data and then imputing them based on the non-missing values in the neighbourhood.\n",
    "\n",
    "The fundamental weakness of KNN doesn't work on categorical features. We need to convert them into numeric using any encoding method. \n",
    "It requires normalizing data as KNN Imputer is a distance-based imputation method and different scales of data generate biased replacements \n",
    "for the missing values.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "df['feature_name'] = knn_imputer.fit_transform(df[['feature_name']])\n",
    "df = pd.DataFrame(knn_imputer.fit_transform(df),columns = df.columns) # for whole dataset\n",
    "\n",
    "# Imputation Using Multivariate Imputation by Chained Equation(MICE)\n",
    "\"\"\"This type of imputation works by filling the missing data multiple times. Multiple Imputations (MIs) are much better than a single \n",
    "imputation as it measures the uncertainty of the missing values in a better way. The chained equations approach is also very flexible and can\n",
    " handle different variables of different data types (ie., continuous or binary) as well as complexities such as bounds or survey skip patterns. \n",
    "\"\"\"\n",
    "\n",
    "from impyute.imputation.cs import mice\n",
    "imputed_training=mice(df.values)\n",
    "\n",
    "# In sklearn, it is implemented as follows\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "mice_imputer = IterativeImputer()\n",
    "df['feature_name'] = mice_imputer.fit_transform(df[['feature_name']])\n",
    "\n",
    "# Stochastic regression imputation:\n",
    "\"\"\"It is quite similar to regression imputation which tries to predict the missing values by regressing it from other related variables\n",
    " in the same dataset plus some random residual value.\"\"\"\n",
    "\n",
    "# Extrapolation and Interpolation:\n",
    "\"\"\"It tries to estimate values from other observations within the range of a discrete set of known data points.\"\"\"\n",
    "\n",
    "# Hot-Deck imputation:\n",
    "\"\"\"Works by randomly choosing the missing value from a set of related and similar variables.\"\"\"\n",
    "\n",
    "# Cold-Deck Imputation: \n",
    "\"\"\"A systematically chosen value from an individual who has similar values on other variables.This is similar to Hot Deck in most ways,\n",
    " but removes the random variation.\"\"\"\n",
    "\n",
    " # Replacing Values using backward & forward filling \n",
    "\n",
    " df.fillna(method = 'bfill', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot outlier detection\n",
    "\n",
    "for var in 'feature name list':\n",
    "    sns.boxplot(df[var])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Treatment\n",
    "\n",
    "# DELETING OBSERVATIONS:\n",
    "\"\"\"We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. \n",
    "We can also use trimming at both ends to remove outliers. But deleting the observation is not a good idea when we have small dataset.\"\"\"\n",
    "\n",
    "for var in 'list of feature name' :\n",
    "    Q1 = df[var].quantile(0.25)\n",
    "    Q3 = df[var].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "df = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "\n",
    "# TRANSFORMING VALUES:\n",
    "\n",
    "# Scalling\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "df['feature_name'] = scaler.fit_transform(df['feature_name'].values.reshape(-1,1))\n",
    "\n",
    "# Log Transformation\n",
    "df['feature_name'] = np.log(df['feature_name'])\n",
    "\n",
    "# Box-transformation\n",
    "import scipy\n",
    "df['feature_name'],fitted_lambda= scipy.stats.boxcox(df['feature_name'] ,lmbda=None)\n",
    "\n",
    "# IMPUTATION\n",
    "q1 = df['feature_name'].quantile(0.25)\n",
    "q3 = df['feature_name'].quantile(0.75)\n",
    "iqr = q3-q1\n",
    "Lower_tail = q1 - 1.5 * iqr\n",
    "Upper_tail = q3 + 1.5 * iqr\n",
    "m = np.mean(df['feature_name']) # Mean,median,mode\n",
    "for i in df['feature_name']:\n",
    "    if i > Upper_tail or i < Lower_tail:\n",
    "            df['feature_name'] = df['feature_name'].replace(i, m)\n",
    "\n",
    "# Binning\n",
    "\"\"\" Binning the data and categorizing them will totally avoid the outliers. It will make the data categorical instead.\"\"\"\n",
    "\n",
    "df['feature_name'] = pd.cut(df['feature_name'], bins = [0, 10, 20, 30, 40, 55], labels = ['Very Low', 'Low', 'Average', 'High', 'Very High'])\n",
    "\n",
    "# Quantile based flooring & capping\n",
    "\n",
    "q10 = df['feature_name'].quantile(0.10)\n",
    "q90 = df['feature_name'].quantile(0.90)\n",
    "df[\"feature_name\"] = np.where(df[\"feature_name\"] <q10, q10,df['feature_name'])\n",
    "df[\"feature_name\"] = np.where(df[\"feature_name\"] >q90, q90,df['feature_name'])\n",
    "\n",
    "\n",
    "# Winsorizing\n",
    "\"\"\"Unlike trimming, here we replace the outliers with other values. Common is replacing the outliers on the upper side with 95% percentile\n",
    " value and outlier on the lower side with 5% percentile.\"\"\"\n",
    "\n",
    "import scipy.stats\n",
    "scipy.stats.mstats.winsorize(df['feature_name'],limits=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "feature_col = \"list of feature names\"\n",
    "le = LabelEncoder()\n",
    "for col in feature_col:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    df_test[col] = le.fit_transform(df_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(sparse=False,drop = 'if_binary')\n",
    "\n",
    "transformed_data = onehotencoder.fit_transform(df[['feature_name']])\n",
    "# the above transformed_data is an array so convert it to dataframe and add feature name to the comlum\n",
    "encoded_data = pd.DataFrame(transformed_data, columns=onehotencoder.get_feature_names_out())\n",
    "\n",
    "# now concatenate the original data and the encoded data using pandas\n",
    "df = pd.concat([df, encoded_data], axis=1).drop('feature_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "dataset.groupby('YrSold')['SalePrice'].median().plot()\n",
    "plt.xlabel('Year Sold')\n",
    "plt.ylabel('Median House Price')\n",
    "plt.title(\"House Price vs Year Sold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get Unique value column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### numerical variables- 2 Types\n",
    "##1. Continuous variable and Discrete variable\n",
    "\n",
    "discrete_feature=[feature for feature in numerical_features if len(df[feature].unique())<=25]\n",
    "print(len(discrete_feature))\n",
    "df[discrete_feature].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets find the relationship between Discrete and Sales Price\n",
    "data=dataset.copy()\n",
    "for feature in discrete_feature:\n",
    "    data.groupby(feature)['SalePrice'].median().plot.bar()\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('SalePrice')\n",
    "    plt.title(feature)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical_features=[feature for feature in dataset.columns if dataset[feature].dtypes!='O']\n",
    "\n",
    "## Continuous Variable\n",
    "continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature+Year_Feature]\n",
    "print(len(continuous_feature))\n",
    "\n",
    "## Lets find the relationship between numerical and Sales Price\n",
    "data = dataset.copy()\n",
    "for feature in continuous_feature:\n",
    "    data.groupby(feature)['SalePrice'].median().plot.hist()\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('SalePrice')\n",
    "    plt.title(feature)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#EDA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will be using logarithmic transformation\n",
    "data=dataset.copy()\n",
    "for feature in continuous_feature:\n",
    "    if 0 in data[feature].unique():\n",
    "        pass\n",
    "    else:\n",
    "        data[feature]=np.log(data[feature])\n",
    "        plt.scatter(data[feature],data['SalePrice'])\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('SalesPrice')\n",
    "        plt.title(feature)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Outliers \n",
    "for feature in continuous_feature:\n",
    "    if 0 in data[feature].unique():\n",
    "        pass\n",
    "    else:\n",
    "        data[feature]=np.log(data[feature])\n",
    "        data.boxplot(column=feature)\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(feature)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['SalePrice'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(dataset['SalePrice'],kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "788a82b91c0336814844834dbe028694862d9bc55d1fd4a8a9273fa14df8fc57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
