{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Import\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# magic function is to enable the inline plotting\n",
    "%matplotlib inline \n",
    "\n",
    "# for display all the column in the datafarmes\n",
    "pd.pandas.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "df= pd.read_csv(\"2019.csv\")\n",
    "# To show first 5 rows of data \n",
    "df.head()\n",
    "# To get shape of data\n",
    "df.shape\n",
    "# describe basic statistics of data (including cat and num)\n",
    "df.describe(include='all')\n",
    "# information about data frame\n",
    "df.info()\n",
    "# data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To seperate categorical and numerical columns\n",
    "features = df.columns\n",
    "cat = df.select_dtypes(include= ['object','category'])\n",
    "num =df.select_dtypes(exclude = ['object','category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Values\n",
    "for var in features:\n",
    "    if len(df[var].unique()) < 7 :\n",
    "        print(var,'-->',len(df[var].unique()),':', df[var].unique())\n",
    "    else :\n",
    "        print(var,'-->',len(df[var].unique()))\n",
    "\n",
    "# To get unique value count\n",
    "df['features_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Representation of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze categorical Columns:\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(4,2,figsize=(12,15))\n",
    "for idx,cat_col in enumerate(cat.columns):\n",
    "    row,col = idx//2,idx%2\n",
    "    sns.countplot(x=cat_col,data=df,hue='traget_feature',ax=axes[row,col])\n",
    "\n",
    "plt.subplots_adjust(hspace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To analyze Numerical Columns:\n",
    "\n",
    "fig,axes = plt.subplots(1,3,figsize=(17,5))\n",
    "for idx,cat_col in enumerate(num.columns):\n",
    "    sns.boxplot(y=cat_col,data=df,x='traget_feature',ax=axes[idx])\n",
    "\n",
    "print(df[num.columns].describe())\n",
    "plt.subplots_adjust(hspace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-Relation Heatmap\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', annot=True, fmt='.1f', linewidths=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot\n",
    "for var in ['features_name']:\n",
    "    sns.boxplot(df[var])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for seeing distrubution of data\n",
    "for var in ['feature names']:\n",
    "    fig= plt.subplots(figsize=(12, 8))\n",
    "    sns.distplot(df[var])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "df.groupby('features_name')['traget_feature'].median().plot()\n",
    "plt.xlabel('features_name')\n",
    "plt.ylabel('Median features_name')\n",
    "plt.title(\"features_name vs traget_feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Completely at Random(MCAR):\n",
    "\"\"\" A variable is missing completely at random (MCAR)if the missing values on a given variable (Y) don't have a relationship with other \n",
    "variables in a given data set or with the variable (Y) itself. In other words, When data is MCAR, there is no relationship between the data\n",
    "missing and any values, and there is no particular reason for the missing values.\"\"\"\n",
    "\n",
    "# Missing at Random(MAR):\n",
    "\"\"\" Let's understands the following examples:\n",
    "Women are less likely to talk about age and weight than men.\n",
    "Men are less likely to talk about salary and emotions than women.\n",
    "familiar right?… This sort of missing content indicates missing at random.\n",
    "\n",
    "MAR occurs when the missingness is not random, but there is a systematic relationship between missing values and other observed data but not\n",
    "the missing data.\n",
    "Let me explain to you: you are working on a dataset of ABC survey. You will find out that many emotion observations are null.\n",
    "You decide to dig deeper and found most of the emotion observations are null that belongs to men's observation.\"\"\"\n",
    "\n",
    "# Missing Not at Random(MNAR):\n",
    "\"\"\" The final and most difficult situation of missingness. MNAR occurs when the missingness is not random, and there is a systematic relationship\n",
    "between missing value, observed value, and missing itself. To make sure, If the missingness is in 2 or more variables holding the same pattern,\n",
    "you can sort the data with one variable and visualize it.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting missing data\n",
    "mis_val =df.isna().sum()\n",
    "mis_val_per = df.isna().sum()/len(df)*100\n",
    "mis_val_table = pd.concat([mis_val, mis_val_per], axis=1)\n",
    "mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "       mis_val_table_ren_columns.iloc[:,:] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get column name which has null value\n",
    "na_features = [var for var in df.columns if df[var].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding reason for missing data using plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Graphical view of missing data\n",
    "\"\"\"The msno.matrix() is a nullity matrix that will help to visualize the location of the null observations.\"\"\"\n",
    "\n",
    "import missingno as msno\n",
    "msno.matrix(df)\n",
    "\n",
    "# To sort graph using one value\n",
    "\"\"\"The missingno package additionally lets us sort the chart by a selective column. Let's sort the value by one feature name \n",
    "column to detect if there is a pattern in the missing values.\"\"\"\n",
    "\n",
    "sorted = df.sort_values('feature_name')\n",
    "msno.matrix(sorted)\n",
    "\n",
    "# Heatmap for Missing Value\n",
    "\"\"\"msno. heatmap() helps to visualize the correlation between missing features. \n",
    "The missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the \n",
    "presence of another\n",
    "\n",
    "Nullity correlation ranges from -1 (if one variable appears the other definitely does not) to 0 (variables appearing or not \n",
    "appearing have no effect on one another) to 1 (if one variable appears the other definitely also does)\"\"\"\n",
    "\n",
    "msno.heatmap(df)\n",
    "\n",
    "# Dendrogram for missing value\n",
    "\n",
    "msno.dendrogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Case Analysis(CCA):\n",
    "\"\"\"This is a quite straightforward method of handling the Missing Data, which directly removes the rows that have missing data \n",
    "i.e we consider only those rows where we have complete data i.e data is not missing. This method is also \n",
    "popularly known as “Listwise deletion”.\n",
    "\n",
    "When to Use:-\n",
    "> Data is MAR(Missing At Random).\n",
    "> Good for Mixed, Numerical, and Categorical data.\n",
    "> Missing data is not more than 5% - 6% of the dataset.\n",
    "> Data doesn't contain much information and will not bias the dataset.\n",
    "\"\"\"\n",
    "\n",
    "df.dropna(subset=['fature_name'],how='any',axis = 0) # Drop rows which contains any NaN or missing value for feature_name column & for complete df remove subset and how\n",
    "\n",
    "## Imputations Techniques for non Time Series Problems:\n",
    "\n",
    "# Arbitrary Value Imputation\n",
    "\"\"\"This is an important technique used in Imputation as it can handle both the Numerical and Categorical variables. \n",
    "This technique states that we group the missing values in a column and assign them to a new value that is far away from the range of that column.\n",
    "Mostly we use values like 99999999 or -9999999 or “Missing” or “Not defined” for numerical & categorical variables.\n",
    "\n",
    "When to Use:-\n",
    "> When data is not MAR(Missing At Random).\n",
    "> Suitable for All.\n",
    "\"\"\"\n",
    "\n",
    "df['features_name'].fillna('Missing')\n",
    "\n",
    "\n",
    "# Frequent Category Imputation\n",
    "\"\"\"\"This technique says to replace the missing value with the variable with the highest frequency or in simple words replacing the values \n",
    "with the Mode of that column. This technique is also referred to as Mode\n",
    "\n",
    "When to Use:-\n",
    "> Data is Missing at Random(MAR)\n",
    "> Missing data is not more than 5% - 6% of the dataset.\n",
    "\"\"\"\n",
    "\n",
    "df['features_name'].fillna(df['features_name'].mode())\n",
    "\n",
    "## Imputations Techniques for Time Series Problems:\n",
    "\n",
    "# Imputing using ffill\n",
    "\n",
    "df.fillna(method='ffill')\n",
    "\n",
    "# Imputation using bfill\n",
    "\n",
    "df.fillna(method='bfill')\n",
    "\n",
    "# Imputation using Linear Interpolation method\n",
    "\n",
    "\"\"\"Linear interpolation is an imputation technique that assumes a linear relationship between data points and utilises non-missing values\n",
    " from adjacent data points to compute a value for a missing data point.\"\"\"\n",
    "\n",
    "df.interpolate(limit_direction=\"both\")\n",
    "\n",
    "## Advanced Imputation Techniques:\n",
    "\n",
    "# Imputation Using k-NN\n",
    "\"\"\"The k nearest neighbours is an algorithm that is used for simple classification. The algorithm uses 'feature similarity' to predict \n",
    "the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the\n",
    "training set. This can be very useful in making predictions about the missing values by finding the k's closest neighbours to the \n",
    "observation with missing data and then imputing them based on the non-missing values in the neighbourhood.\n",
    "\n",
    "The fundamental weakness of KNN doesn't work on categorical features. We need to convert them into numeric using any encoding method. \n",
    "It requires normalizing data as KNN Imputer is a distance-based imputation method and different scales of data generate biased replacements \n",
    "for the missing values.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "df['feature_name'] = knn_imputer.fit_transform(df[['feature_name']])\n",
    "df = pd.DataFrame(knn_imputer.fit_transform(df),columns = df.columns) # for whole dataset\n",
    "\n",
    "# Imputation Using Multivariate Imputation by Chained Equation(MICE)\n",
    "\"\"\"This type of imputation works by filling the missing data multiple times. Multiple Imputations (MIs) are much better than a single \n",
    "imputation as it measures the uncertainty of the missing values in a better way. The chained equations approach is also very flexible and can\n",
    " handle different variables of different data types (ie., continuous or binary) as well as complexities such as bounds or survey skip patterns. \n",
    "\"\"\"\n",
    "\n",
    "from impyute.imputation.cs import mice\n",
    "imputed_training=mice(df.values)\n",
    "\n",
    "# In sklearn, it is implemented as follows\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "mice_imputer = IterativeImputer()\n",
    "df['feature_name'] = mice_imputer.fit_transform(df[['feature_name']])\n",
    "\n",
    "# Stochastic regression imputation:\n",
    "\"\"\"It is quite similar to regression imputation which tries to predict the missing values by regressing it from other related variables\n",
    " in the same dataset plus some random residual value.\"\"\"\n",
    "\n",
    "# Extrapolation and Interpolation:\n",
    "\"\"\"It tries to estimate values from other observations within the range of a discrete set of known data points.\"\"\"\n",
    "\n",
    "# Hot-Deck imputation:\n",
    "\"\"\"Works by randomly choosing the missing value from a set of related and similar variables.\"\"\"\n",
    "\n",
    "# Cold-Deck Imputation: \n",
    "\"\"\"A systematically chosen value from an individual who has similar values on other variables.This is similar to Hot Deck in most ways,\n",
    " but removes the random variation.\"\"\"\n",
    "\n",
    " # Replacing Values using backward & forward filling \n",
    "\n",
    "df.fillna(method = 'bfill', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot outlier detection\n",
    "\n",
    "for var in 'feature name list':\n",
    "    sns.boxplot(df[var])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Treatment\n",
    "\n",
    "# DELETING OBSERVATIONS:\n",
    "\"\"\"We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. \n",
    "We can also use trimming at both ends to remove outliers. But deleting the observation is not a good idea when we have small dataset.\"\"\"\n",
    "\n",
    "for var in 'list of feature name' :\n",
    "    Q1 = df[var].quantile(0.25)\n",
    "    Q3 = df[var].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "df = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "\n",
    "# TRANSFORMING VALUES:\n",
    "\n",
    "# Scalling\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "df['feature_name'] = scaler.fit_transform(df['feature_name'].values.reshape(-1,1))\n",
    "\n",
    "# Log Transformation\n",
    "df['feature_name'] = np.log(df['feature_name'])\n",
    "\n",
    "# Box-transformation\n",
    "import scipy\n",
    "df['feature_name'],fitted_lambda= scipy.stats.boxcox(df['feature_name'] ,lmbda=None)\n",
    "\n",
    "# IMPUTATION\n",
    "q1 = df['feature_name'].quantile(0.25)\n",
    "q3 = df['feature_name'].quantile(0.75)\n",
    "iqr = q3-q1\n",
    "Lower_tail = q1 - 1.5 * iqr\n",
    "Upper_tail = q3 + 1.5 * iqr\n",
    "m = np.mean(df['feature_name']) # Mean,median,mode\n",
    "for i in df['feature_name']:\n",
    "    if i > Upper_tail or i < Lower_tail:\n",
    "            df['feature_name'] = df['feature_name'].replace(i, m)\n",
    "\n",
    "# Binning\n",
    "\"\"\" Binning the data and categorizing them will totally avoid the outliers. It will make the data categorical instead.\"\"\"\n",
    "\n",
    "df['feature_name'] = pd.cut(df['feature_name'], bins = [0, 10, 20, 30, 40, 55], labels = ['Very Low', 'Low', 'Average', 'High', 'Very High'])\n",
    "\n",
    "# Quantile based flooring & capping\n",
    "\n",
    "q10 = df['feature_name'].quantile(0.10)\n",
    "q90 = df['feature_name'].quantile(0.90)\n",
    "df[\"feature_name\"] = np.where(df[\"feature_name\"] <q10, q10,df['feature_name'])\n",
    "df[\"feature_name\"] = np.where(df[\"feature_name\"] >q90, q90,df['feature_name'])\n",
    "\n",
    "\n",
    "# Winsorizing\n",
    "\"\"\"Unlike trimming, here we replace the outliers with other values. Common is replacing the outliers on the upper side with 95% percentile\n",
    " value and outlier on the lower side with 5% percentile.\"\"\"\n",
    "\n",
    "import scipy.stats\n",
    "scipy.stats.mstats.winsorize(df['feature_name'],limits=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding using map function\n",
    "feature_name_dict = {'N':0, 'Y':1}\n",
    "df['feature_name'] = df['feature_name'].map(feature_name_dict)\n",
    "\n",
    "# Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "feature_col = \"list of feature names\"\n",
    "le = LabelEncoder()\n",
    "for col in feature_col:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# One Hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(sparse=False,drop = 'if_binary')\n",
    "transformed_data = onehotencoder.fit_transform(df[['feature_name']])\n",
    "# the above transformed_data is an array so convert it to dataframe and add feature name to the comlum\n",
    "encoded_data = pd.DataFrame(transformed_data, columns=onehotencoder.get_feature_names_out())\n",
    "# now concatenate the original data and the encoded data using pandas\n",
    "df = pd.concat([df, encoded_data], axis=1).drop('feature_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "na_features = ['feature names']\n",
    "for var in na_features:\n",
    "    df[var] = scaler.fit_transform(df[var].values.reshape(-1, 1))\n",
    "    df_test[var] = scaler.transform(df_test[var].values.reshape(-1, 1))\n",
    "\n",
    "# maximum absolute scaling\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "max_scaler = MaxAbsScaler()\n",
    "for var in na_features:\n",
    "    df[var] = max_scaler.fit_transform(df[var].values.reshape(-1, 1))\n",
    "    df_test[var] = max_scaler.transform(df_test[var].values.reshape(-1, 1))  \n",
    "\n",
    "#  min-max feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm_scaler = MinMaxScaler()\n",
    "for var in na_features:\n",
    "    df[var] = mm_scaler.fit_transform(df[var].values.reshape(-1, 1))\n",
    "    df_test[var] = mm_scaler.transform(df_test[var].values.reshape(-1, 1))  \n",
    "\n",
    "# log scaling  \n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "ln = FunctionTransformer(np.log1p)\n",
    "for var in na_features:\n",
    "    df[var] = ln.fit_transform(df[var].values.reshape(-1, 1))\n",
    "    df_test[var] = ln.transform(df_test[var].values.reshape(-1, 1))\n",
    "\n",
    "# Normalize\n",
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer()\n",
    "for var in na_features:\n",
    "    df[var] = norm.fit_transform(df[var].values.reshape(-1, 1))\n",
    "    df_test[var] = norm.transform(df_test[var].values.reshape(-1, 1))\n",
    "\n",
    "# RobustScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robu = RobustScaler()\n",
    "for var in na_features:\n",
    "    df[var] = robu.fit_transform(df[var].values.reshape(-1, 1))\n",
    "    df_test[var] = robu.transform(df_test[var].values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generlized Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(['Traget_feature'], axis=1)\n",
    "y = df[['Traget_feature']]\n",
    "X_test = df_test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr=linear_model.LinearRegression()\n",
    "lm_model=lr.fit(X_train,y_train)\n",
    "y_pred=lm_model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val,y_pred))\n",
    "\n",
    "# XGBRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "xgb_reg = XGBRegressor(n_estimators=400, max_depth = 20, learning_rate = 0.05,  min_child_weight  = 40)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_pred, y_val))\n",
    "print (xgb_reg)\n",
    "\n",
    "# RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "random_forest_regressor = RandomForestRegressor(n_estimators = 10, random_state = 27)\n",
    "random_forest_regressor.fit(X_train, y_train)\n",
    "Y_pred = random_forest_regressor.predict(X_val)\n",
    "mse = mean_squared_error(y_val, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "tree_model = DecisionTreeRegressor(random_state=1)\n",
    "tree_model.fit(X_train, y_train)\n",
    "Y_pred = tree_model.predict(X_val)\n",
    "mse = mean_squared_error(y_val, Y_pred)\n",
    "rmse = np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logistic_model = LogisticRegression(random_state=1)\n",
    "lg_model = logistic_model.fit(X_train,y_train)\n",
    "y_pred_logistic=lg_model.predict(X_val)\n",
    "score_logistic =accuracy_score(y_pred_logistic,y_val)*100\n",
    "\n",
    "# DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_model = DecisionTreeClassifier(random_state=1)\n",
    "tree_model.fit(X_train,y_train)\n",
    "pred_cv_tree=tree_model.predict(X_val)\n",
    "score_tree =accuracy_score(pred_cv_tree,y_val)*100 \n",
    "\n",
    "# RandomForestClassifier with grid_search\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "paramgrid = {'max_depth': list(range(1,20,2)),'n_estimators':list(range(1,200,20))}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=1),paramgrid)\n",
    "grid_search.fit(X_train,y_train)\n",
    "grid_search.best_estimator_\n",
    "\n",
    "forest_model = RandomForestClassifier(random_state=1,max_depth=10,n_estimators=50) # add values from grid_search.best_estimator_\n",
    "forest_model.fit(X_train,y_train)\n",
    "pred_cv_forest=forest_model.predict(X_val)\n",
    "score_forest = accuracy_score(pred_cv_forest,y_val)*100\n",
    "\n",
    "# XGBClassifier\n",
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier(n_estimators=50,max_depth=4) \n",
    "xgb_model.fit(X_train,y_train)\n",
    "pred_xgb=xgb_model.predict(X_val)\n",
    "score_xgb = accuracy_score(pred_xgb,y_val)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot feature importance\n",
    "def plot_features(booster, figsize):    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plot_features(model_name, (10,14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for saving all model performance in one place\n",
    "from collections import OrderedDict\n",
    "model_performance = OrderedDict()\n",
    "\n",
    "# This line for all model performance\n",
    "model_performance['Model name'] = round(rmse,3)\n",
    "print(f'Root Mean Squared Error of the model is : {round(rmse,3)}')\n",
    "\n",
    "# For showing model performance\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save data into other \n",
    "testRes = df[['feature_name']]\n",
    "testRes['feature_name'] = df['feature_name']\n",
    "testRes\n",
    "\n",
    "# To predict and save values in columns\n",
    "yPreds = model_name.predict(X_test)\n",
    "testRes['target_name'] = yPreds\n",
    "submission = testRes[['feature_name & Traget_name to be saved in file']]\n",
    "\n",
    "# To SAve submission file\n",
    "submission.columns = ['feature_name & Traget_name to be saved in file']\n",
    "submission.to_csv('submission.csv', index = False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date time dtype conversion\n",
    "df['Datetime']=pd.to_datetime(df.Datetime, format='%d-%m-%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Time Based Feature.This helps regression models to understand the trend in the data.\n",
    "\n",
    "# Below function extracts date related features from datetime\n",
    "def create_date_featues(df):\n",
    "    df['Year'] = pd.to_datetime(df['DateTime']).dt.year\n",
    "    df['Month'] = pd.to_datetime(df['DateTime']).dt.month\n",
    "    df['Day'] = pd.to_datetime(df['DateTime']).dt.day\n",
    "    df['Dayofweek'] = pd.to_datetime(df['DateTime']).dt.dayofweek\n",
    "    df['DayOfyear'] = pd.to_datetime(df['DateTime']).dt.dayofyear\n",
    "    df['Week'] = pd.to_datetime(df['DateTime']).dt.week\n",
    "    df['Quarter'] = pd.to_datetime(df['DateTime']).dt.quarter \n",
    "    df['Is_month_start'] = pd.to_datetime(df['DateTime']).dt.is_month_start\n",
    "    df['Is_month_end'] = pd.to_datetime(df['DateTime']).dt.is_month_end\n",
    "    df['Is_quarter_start'] = pd.to_datetime(df['DateTime']).dt.is_quarter_start\n",
    "    df['Is_quarter_end'] = pd.to_datetime(df['DateTime']).dt.is_quarter_end\n",
    "    df['Is_year_start'] = pd.to_datetime(df['DateTime']).dt.is_year_start\n",
    "    df['Is_year_end'] = pd.to_datetime(df['DateTime']).dt.is_year_end\n",
    "    df['Semester'] = np.where(df['Quarter'].isin([1,2]),1,2)\n",
    "    df['Is_weekend'] = np.where(df['Dayofweek'].isin([5,6]),1,0)\n",
    "    df['Is_weekday'] = np.where(df['Dayofweek'].isin([0,1,2,3,4]),1,0)\n",
    "    df['Hour'] = pd.to_datetime(df['DateTime']).dt.hour\n",
    "    \n",
    "    return df\n",
    "\n",
    "# extracting time related \n",
    "df=create_date_featues(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert datetime col into index\n",
    "indexed_df = df.set_index('Datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plots in the notebook\n",
    "%matplotlib inline\n",
    "indexed_df['traget_feature'].plot(figsize=(12,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,['feature_name','traget_feature']].plot(x='feature_name',y='traget_feature',title='traget_feature Trend',figsize=(16,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test split\n",
    "Timeseries problems requires time based validation instead of generaly used kfold validation in regression problem. Kfold splits the data randomly and checking the model accuracy by predicting on timeperiod 2016 by using 2017 data makes no sense.\n",
    "insted we use time based validation for the time period (2017-01-01 to 2017-04-01) of 4 months, since the test set contains 4 months data to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1=indexed_df[ :'2014-02-25 23:00:00']#Train period from 2016-01-01 to 2017-02-31\n",
    "val1=indexed_df['2014-02-25 23:00:00': ] #Month 3,4,5,6 as validtaion period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 aug:\n",
    "7pm:logistic\n",
    "8pm:pca\n",
    "8.45:time_series\n",
    "24 aug:\n",
    "8pm:SQL-ETL\n",
    "25 aug:\n",
    "8pm:SQL-ETL\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "788a82b91c0336814844834dbe028694862d9bc55d1fd4a8a9273fa14df8fc57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
